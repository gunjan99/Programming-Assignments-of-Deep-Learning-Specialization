{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your Deep Neural Network: Step by Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhenhui\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v3 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    A = np.maximum(0,Z)\n",
    "\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Outline of the Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 2-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(3, 2, 1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters[\"W\" + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters[\"b\" + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forward propagation module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Linear Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Linear-Activation Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    assert(A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Compute loss from AL and y.\n",
    "    cost = - 1 / m * np.sum(np.multiply(np.log(AL), Y) + np.multiply(np.log(1 - AL), 1 - Y))\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Backward propagation module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Linear backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Linear-Activation backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "AL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "\n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L - 1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 2)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for Image Classification: Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from dnn_app_utils_v2 import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 0. It's a non-cat picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 15\n",
    "plt.imshow(train_x_orig[index])\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 209\n",
      "Number of testing examples: 50\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_x_orig shape: (209, 64, 64, 3)\n",
      "train_y shape: (1, 209)\n",
      "test_x_orig shape: (50, 64, 64, 3)\n",
      "test_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "# Explore your dataset\n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten / 255.\n",
    "test_x = test_x_flatten / 255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = 12288     # num_px * num_px * 3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: two_layer_model\n",
    "\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "\n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []\n",
    "    m = X.shape[1] # to keep track of the cost\n",
    "    (n_x, n_h, n_y) = layers_dims # number of examples\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    parameters = initialize_parameters(n_x, n_y, n_h)\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1\". Output: \"A1, cache1, A2, cache2\".\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\")\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 10 == 0:\n",
    "            costs.append(np.squeeze(cost))\n",
    "    \n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-f2297765406a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtwo_layer_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-12b1c2b41085>\u001b[0m in \u001b[0;36mtwo_layer_model\u001b[1;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# Compute cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# Initializing backward propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Workspace\\Deeplearning.ai编程作业\\神经网络和深度学习\\Building your DNN & DNN for Image Classification Application\\dnn_app_utils_v2.py\u001b[0m in \u001b[0;36mcompute_cost\u001b[1;34m(AL, Y)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m      \u001b[1;31m# To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m     \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
